{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Sample data entries, each containing function definition, test cases, and expected results.\n",
    "entries = [\n",
    "    {\n",
    "        \"task_id\": \"HumanEval/0\",\n",
    "        \"prompt\": \"from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    ...\",\n",
    "        \"canonical_solution\": \"    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n\",\n",
    "        \"test\": \"assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\nassert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n...\",\n",
    "        \"entry_point\": \"has_close_elements\"\n",
    "    },\n",
    "    # More entries like HumanEval/1, HumanEval/2, etc.\n",
    "]\n",
    "\n",
    "# Define a helper to run each function and compare with expected results.\n",
    "def run_tests(entry):\n",
    "    # Prepare the function\n",
    "    function_code = entry[\"prompt\"] + entry[\"canonical_solution\"]\n",
    "    function_name = entry[\"entry_point\"]\n",
    "    \n",
    "    # Dynamically define the function\n",
    "    exec(function_code, globals())\n",
    "    \n",
    "    # Define candidate function as per the entry_point name\n",
    "    candidate = globals()[function_name]\n",
    "    \n",
    "    # Prepare and execute tests\n",
    "    test_code = entry[\"test\"]\n",
    "    try:\n",
    "        # Run test cases\n",
    "        exec(test_code, {\"candidate\": candidate})\n",
    "        print(f\"{entry['task_id']} passed all tests.\")\n",
    "    except AssertionError:\n",
    "        print(f\"{entry['task_id']} failed one or more tests.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful tests: 164\n",
      "Number of failed tests: 0\n",
      "Detailed test results with extracted code saved to 'test_results_with_code.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"results-fill-missing/result-meta-llama-Llama-3.1-70B-Instruct.csv\")\n",
    "\n",
    "# Function to run each test case and determine if it passes or fails\n",
    "def run_tests(row):\n",
    "    function_code = row[\"prompt\"] + \"\\n\" + row[\"canonical_solution\"]\n",
    "    function_name = row[\"entry_point\"]\n",
    "    test_code = row[\"test\"]\n",
    "\n",
    "    # Define the function dynamically in the global scope\n",
    "    exec(function_code, globals())\n",
    "    candidate = globals().get(function_name)  # Access the function by name\n",
    "\n",
    "    # Run the test cases and determine result\n",
    "    try:\n",
    "        exec(test_code, {\"candidate\": candidate})\n",
    "        return \"Passed\"\n",
    "    except AssertionError:\n",
    "        return \"Failed\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Apply test function to each row and store result in a new column\n",
    "df[\"result\"] = df.apply(run_tests, axis=1)\n",
    "\n",
    "# Count the number of successes and failures\n",
    "success_count = (df[\"result\"] == \"Passed\").sum()\n",
    "failure_count = len(df) - success_count\n",
    "\n",
    "print(f\"Number of successful tests: {success_count}\")\n",
    "print(f\"Number of failed tests: {failure_count}\")\n",
    "\n",
    "# Extract the generated code for each row\n",
    "df[\"extracted_code\"] = df[\"canonical_solution\"]\n",
    "\n",
    "# Save the DataFrame with test results and extracted code to a new CSV file\n",
    "df.to_csv(\"test_results_with_code.csv\", columns=[\"task_id\", \"result\", \"extracted_code\"], index=False)\n",
    "\n",
    "# Display summary\n",
    "print(\"Detailed test results with extracted code saved to 'test_results_with_code.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
